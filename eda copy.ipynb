{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.schema import BaseRetriever  # or from langchain.retrievers.base import BaseRetriever\n",
    "from langchain.schema import BaseRetriever\n",
    "from typing import Any, List, Tuple\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import OpenAI, HuggingFaceHub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "import getpass\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elisimicrobertson/Documents/relevance_asssessment/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO: pikepdf C++ to Python logger bridge initialized\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import OpenAIEmbeddings, CohereEmbeddings\n",
    "\n",
    "load_dotenv()  # Load the environment variables from .env\n",
    "\n",
    "def get_embedding_model(model_name: str):\n",
    "    \"\"\"\n",
    "    Factory function to return an embedding model instance.\n",
    "    \n",
    "    Supported model_name values:\n",
    "      - \"openai\": Returns an OpenAIEmbeddings instance.\n",
    "      - \"cohere\": Returns a CohereEmbeddings instance.\n",
    "    \"\"\"\n",
    "    if model_name.lower() == \"openai\":\n",
    "        openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not openai_api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set in the environment.\")\n",
    "        return OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    elif model_name.lower() == \"cohere\":\n",
    "        cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "        if not cohere_api_key:\n",
    "            raise ValueError(\"COHERE_API_KEY is not set in the environment.\")\n",
    "        return CohereEmbeddings(cohere_api_key=cohere_api_key)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported embedding model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d_/d28rwb_10zd4841zb_1t_fmw0000gn/T/ipykernel_88448/1973275175.py:19: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  return OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: Loading faiss.\n",
      "INFO: Successfully loaded faiss.\n",
      "INFO: Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the key coverages in the insurance policy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d_/d28rwb_10zd4841zb_1t_fmw0000gn/T/ipykernel_88448/758316775.py:18: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"), model=\"gpt-4o\")\n"
     ]
    }
   ],
   "source": [
    "embedding_model = get_embedding_model(\"openai\")\n",
    "\n",
    "# Build the dense vector store using FAISS.\n",
    "# FAISS provides similarity_search_with_score which we'll use in our hybrid retriever.\n",
    "vector_store = FAISS.from_documents(elements, embedding_model)\n",
    "\n",
    "dense_retriever = vector_store\n",
    "\n",
    "# Initialize BM25 retriever\n",
    "bm25_retriever = BM25Retriever(elements)\n",
    "\n",
    "# Create a HybridRetriever instance with a chosen weight.\n",
    "# For example, alpha=0.5 gives equal weight to dense and BM25 scores.\n",
    "hybrid_retriever = HybridRetriever(bm25_retriever=bm25_retriever, \n",
    "                                   dense_retriever=dense_retriever,\n",
    "                                   alpha=0.5)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"), model=\"gpt-4o\")\n",
    "\n",
    "query = \"What are the key coverages in the insurance policy?\"\n",
    "print(\"Query:\", query)\n",
    "\n",
    "qa_pipeline = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=hybrid_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "judge_llm = init_chat_model(\"gpt-4o-mini\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"), model_provider=\"openai\")\n",
    "judge_llm = judge_llm.with_structured_output(JudgeResponse)\n",
    "results = llm_judge_evaluation(dataset, qa_pipeline, judge_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d_/d28rwb_10zd4841zb_1t_fmw0000gn/T/ipykernel_88448/2639158125.py:121: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  documents = self.retriever.get_relevant_documents(question)\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Initialize the chain\n",
    "qa_chain = LongQAChain(\n",
    "    retriever=hybrid_retriever,\n",
    "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "answer = qa_chain.run(\"Summarise the policies of this document?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
