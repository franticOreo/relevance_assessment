{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.schema import BaseRetriever  # or from langchain.retrievers.base import BaseRetriever\n",
    "from langchain.schema import BaseRetriever\n",
    "from typing import Any, List, Tuple\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import OpenAI, HuggingFaceHub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "import getpass\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elisimicrobertson/Documents/relevance_asssessment/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO: pikepdf C++ to Python logger bridge initialized\n"
     ]
    }
   ],
   "source": [
    "# Configure the loader with parameters to handle different elements\n",
    "loader = UnstructuredLoader(\n",
    "    \"./docs/nrma-car-pds-spds007-1023-nsw-act-qld-tas.pdf\",\n",
    "    chunking_strategy=\"by_title\"\n",
    ")\n",
    "\n",
    "# Load the document\n",
    "elements = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import OpenAIEmbeddings, CohereEmbeddings\n",
    "\n",
    "load_dotenv()  # Load the environment variables from .env\n",
    "\n",
    "def get_embedding_model(model_name: str):\n",
    "    \"\"\"\n",
    "    Factory function to return an embedding model instance.\n",
    "    \n",
    "    Supported model_name values:\n",
    "      - \"openai\": Returns an OpenAIEmbeddings instance.\n",
    "      - \"cohere\": Returns a CohereEmbeddings instance.\n",
    "    \"\"\"\n",
    "    if model_name.lower() == \"openai\":\n",
    "        openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not openai_api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set in the environment.\")\n",
    "        return OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    elif model_name.lower() == \"cohere\":\n",
    "        cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "        if not cohere_api_key:\n",
    "            raise ValueError(\"COHERE_API_KEY is not set in the environment.\")\n",
    "        return CohereEmbeddings(cohere_api_key=cohere_api_key)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported embedding model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Retriever:\n",
    "    def __init__(self, documents):\n",
    "        \"\"\"\n",
    "        Initializes the BM25 retriever.\n",
    "        \n",
    "        Parameters:\n",
    "            documents (list): A list of Document objects. Each document should have a 'page_content' attribute.\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        # Pre-tokenize each document’s content for BM25 indexing.\n",
    "        self.tokenized_corpus = [self._tokenize(doc.page_content) for doc in documents]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "        \n",
    "    def _tokenize(self, text):\n",
    "        \"\"\"\n",
    "        A simple tokenizer that lowercases text and extracts word tokens.\n",
    "        \"\"\"\n",
    "        tokens = re.findall(r\"\\w+\", text.lower())\n",
    "        return tokens\n",
    "    \n",
    "    def get_relevant_documents(self, query: str, k: int = 5):\n",
    "        \"\"\"\n",
    "        Returns a list of Document objects relevant to the query.\n",
    "        The BM25 scores are attached to the document metadata if needed.\n",
    "        \"\"\"\n",
    "        query_tokens = self._tokenize(query)\n",
    "        scores = self.bm25.get_scores(query_tokens)\n",
    "        # Get indices of top k scores.\n",
    "        top_n_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
    "        \n",
    "        docs = []\n",
    "        for idx in top_n_indices:\n",
    "            doc = self.documents[idx]  # This is an instance of Document.\n",
    "            # Optionally, store the BM25 score in the document metadata.\n",
    "            doc.metadata[\"score\"] = scores[idx]\n",
    "            docs.append(doc)\n",
    "        return docs\n",
    "\n",
    "class HybridRetriever(BaseRetriever):\n",
    "    bm25_retriever: Any  # You can replace Any with the specific type if available\n",
    "    dense_retriever: Any\n",
    "    alpha: float = 0.5\n",
    "\n",
    "    def _get_relevant_documents(self, query: str, k: int = 5):\n",
    "        # Get BM25 documents. The BM25 retriever now returns a list of Documents.\n",
    "        bm25_docs = self.bm25_retriever.get_relevant_documents(query, k=k)\n",
    "        \n",
    "        # Retrieve documents from the dense retriever.\n",
    "        # FAISS.similarity_search_with_score returns a list of tuples (doc, score)\n",
    "        dense_results = self.dense_retriever.similarity_search_with_score(query, k=k)\n",
    "        \n",
    "        # Convert dense retriever tuples into documents (with optional score in metadata).\n",
    "        dense_docs = []\n",
    "        for doc, score in dense_results:\n",
    "            # Optionally, attach the dense score:\n",
    "            doc.metadata[\"score\"] = score\n",
    "            dense_docs.append(doc)\n",
    "        \n",
    "        # Combine the two lists.\n",
    "        combined_docs = bm25_docs + dense_docs\n",
    "        \n",
    "        # Improved deduplication using a content-based hash of the normalized page_content.\n",
    "        unique_docs = {}\n",
    "        for doc in combined_docs:\n",
    "            # Normalize the text: strip extra whitespace and lowercase.\n",
    "            normalized_content = doc.page_content.strip().lower()\n",
    "            content_hash = hash(normalized_content)\n",
    "            if content_hash in unique_docs:\n",
    "                # If duplicate, optionally keep the document with the higher score.\n",
    "                if doc.metadata.get(\"score\", 0) > unique_docs[content_hash].metadata.get(\"score\", 0):\n",
    "                    unique_docs[content_hash] = doc\n",
    "            else:\n",
    "                unique_docs[content_hash] = doc\n",
    "\n",
    "        return list(unique_docs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d_/d28rwb_10zd4841zb_1t_fmw0000gn/T/ipykernel_88448/1973275175.py:19: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  return OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: Loading faiss.\n",
      "INFO: Successfully loaded faiss.\n",
      "INFO: Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the key coverages in the insurance policy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d_/d28rwb_10zd4841zb_1t_fmw0000gn/T/ipykernel_88448/758316775.py:18: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"), model=\"gpt-4o\")\n"
     ]
    }
   ],
   "source": [
    "embedding_model = get_embedding_model(\"openai\")\n",
    "\n",
    "# Build the dense vector store using FAISS.\n",
    "# FAISS provides similarity_search_with_score which we'll use in our hybrid retriever.\n",
    "vector_store = FAISS.from_documents(elements, embedding_model)\n",
    "\n",
    "dense_retriever = vector_store\n",
    "\n",
    "# Initialize BM25 retriever\n",
    "bm25_retriever = BM25Retriever(elements)\n",
    "\n",
    "# Create a HybridRetriever instance with a chosen weight.\n",
    "# For example, alpha=0.5 gives equal weight to dense and BM25 scores.\n",
    "hybrid_retriever = HybridRetriever(bm25_retriever=bm25_retriever, \n",
    "                                   dense_retriever=dense_retriever,\n",
    "                                   alpha=0.5)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"), model=\"gpt-4o\")\n",
    "\n",
    "query = \"What are the key coverages in the insurance policy?\"\n",
    "print(\"Query:\", query)\n",
    "\n",
    "qa_pipeline = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=hybrid_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.graph import KnowledgeGraph, Node\n",
    "from ragas.testset.transforms import apply_transforms, Parallel\n",
    "from ragas.testset.transforms.extractors import NERExtractor, KeyphrasesExtractor\n",
    "from ragas.testset.transforms.relationship_builders.traditional import JaccardSimilarityBuilder\n",
    "# from ragas.testset.synthesizers.base_query import QuerySynthesizer, SingleTurnSample\n",
    "from ragas.testset.synthesizers.base import BaseSynthesizer\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "\n",
    "# Step 1: Convert your pre-chunked documents into nodes.\n",
    "# Assume each element in \"elements\" has a \"text\" attribute.\n",
    "nodes = [Node(properties={\"page_content\": element.page_content}) for element in elements]\n",
    "\n",
    "# Step 2: Build the Knowledge Graph from nodes.\n",
    "kg = KnowledgeGraph(nodes=nodes)\n",
    "\n",
    "# Step 3: Define and apply the transformation pipeline.\n",
    "# You can run extractors in parallel and then a relationship builder.\n",
    "ner_extractor = NERExtractor()\n",
    "key_extractor = KeyphrasesExtractor()\n",
    "rel_builder = JaccardSimilarityBuilder(property_name=\"entities\", key_name=\"PER\", new_property_name=\"entity_jaccard_similarity\")\n",
    "\n",
    "transforms = [\n",
    "    Parallel(ner_extractor, key_extractor),\n",
    "    rel_builder\n",
    "]\n",
    "# Apply the transforms asynchronously to enrich the knowledge graph.\n",
    "apply_transforms(kg, transforms)\n",
    "\n",
    "\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\", api_key=os.getenv(\"OPENAI_API_KEY\")))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\")))\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, \n",
    "                             embedding_model=embedding_model, \n",
    "                             knowledge_graph=kg)\n",
    "\n",
    "elementsf = [el for el in elements if len(el.page_content) > 200]\n",
    "\n",
    "dataset = generator.generate_with_langchain_docs(elementsf, testset_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "def llm_judge_evaluation(\n",
    "    dataset: List[Any],\n",
    "    qa_pipeline: Any,\n",
    "    judge_llm: ChatOpenAI\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluates your retriever+LLM pipeline on each sample in the dataset using an LLM judge.\n",
    "    \n",
    "    Parameters:\n",
    "      - dataset: a list of samples. Each sample must have either attributes or keys for:\n",
    "          • 'question': the query text.\n",
    "          • 'solution' or 'reference': the gold standard answer.\n",
    "      - qa_pipeline: your combined QA retrieval pipeline (e.g., an instance of RetrievalQA).\n",
    "      - judge_llm: a ChatOpenAI instance configured as the evaluator.\n",
    "      \n",
    "    Returns:\n",
    "      A dictionary containing individual scores, detailed evaluation info for each sample,\n",
    "      and the overall average score.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    detailed_results = []\n",
    "    \n",
    "    for _, sample in dataset.to_pandas().iterrows():\n",
    "        user_input = sample[\"user_input\"]\n",
    "        reference = sample[\"reference\"]\n",
    "        \n",
    "        # Generate the candidate answer using your QA pipeline.\n",
    "        candidate_answer = qa_pipeline.run(user_input)\n",
    "        \n",
    "        # Construct the judge prompt.\n",
    "        judge_prompt = f\"\"\"\n",
    "        You are an experienced evaluator. Please score the following candidate answer on a Likert scale from 1 to 4, where:\n",
    "            1 = Poor: Incorrect, irrelevant, or incomplete.\n",
    "            2 = Fair: Moderately accurate but missing some details.\n",
    "            3 = Good: Mostly accurate and relevant.\n",
    "            4 = Excellent: Completely accurate and comprehensive.\n",
    "\n",
    "        Question: {user_input}\n",
    "\n",
    "        Candidate Answer: {candidate_answer}\n",
    "\n",
    "        Reference Answer: {reference}\n",
    "\n",
    "        Provide feedback to the candidate answer given the reference answer. Then provide the score.\n",
    "\"\"\"\n",
    "        # Obtain the judge's response.\n",
    "        judge_response = judge_llm.invoke(judge_prompt)\n",
    "\n",
    "        scores.append(judge_response.score)\n",
    "        detailed_results.append({\n",
    "            \"question\": user_input,\n",
    "            \"candidate_answer\": candidate_answer,\n",
    "            \"reference_answer\": reference,\n",
    "            \"judge_response\": judge_response,\n",
    "            \"score\": judge_response.score\n",
    "        })\n",
    "    \n",
    "    average_score = sum(scores) / len(scores) if scores else 0\n",
    "    return {\n",
    "        \"scores\": scores,\n",
    "        \"average_score\": average_score,\n",
    "        \"detailed_results\": detailed_results\n",
    "    }\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "# judge_llm = ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"), model=\"gpt-3.5-turbo\")\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "class JudgeResponse(BaseModel):\n",
    "    feedback: str = Field(description=\"Feedback on the candidate answer\")\n",
    "    score: int = Field(description=\"Score on a scale of 1-4\")\n",
    "\n",
    "judge_llm = init_chat_model(\"gpt-4o-mini\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"), model_provider=\"openai\")\n",
    "judge_llm = judge_llm.with_structured_output(JudgeResponse)\n",
    "results = llm_judge_evaluation(dataset, qa_pipeline, judge_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Sequence\n",
    "from operator import itemgetter\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableSequence, RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class LongQAChain:\n",
    "    \"\"\"A question-answering chain that uses map-reduce to handle long contexts efficiently.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: Optional[BaseLanguageModel] = None,\n",
    "        retriever: Optional[BaseRetriever] = None,\n",
    "        map_prompt: Optional[PromptTemplate] = None,\n",
    "        reduce_prompt: Optional[PromptTemplate] = None\n",
    "    ):\n",
    "        \"\"\"Initialize the QA chain.\"\"\"\n",
    "        self.llm = llm or ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "        self.retriever = retriever\n",
    "        self.map_prompt = map_prompt or self._default_map_prompt()\n",
    "        self.reduce_prompt = reduce_prompt or self._default_reduce_prompt()\n",
    "        self.chain = self._create_chain()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _default_map_prompt() -> PromptTemplate:\n",
    "        \"\"\"Create default mapping prompt.\"\"\"\n",
    "        template = \"\"\"The following is a chunk of an insurance document:\n",
    "        {context}\n",
    "        \n",
    "        Based on this chunk, what information is relevant to answering: {question}\n",
    "        \n",
    "        Relevant information:\"\"\"\n",
    "        \n",
    "        return PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _default_reduce_prompt() -> PromptTemplate:\n",
    "        \"\"\"Create default reducing prompt.\"\"\"\n",
    "        template = \"\"\"Given the following extracted information from different parts of an insurance document, \n",
    "        provide a comprehensive answer to the question: {question}\n",
    "\n",
    "        Extracted information:\n",
    "        {context}\n",
    "\n",
    "        Final Answer:\"\"\"\n",
    "        \n",
    "        return PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "    \n",
    "    def _create_chain(self) -> RunnableSequence:\n",
    "        \"\"\"Create the map-reduce chain using the LangChain v0.3 syntax.\"\"\"\n",
    "        \n",
    "        # Define the map function to process individual documents\n",
    "        map_chain = (\n",
    "            self.map_prompt \n",
    "            | self.llm\n",
    "            | (lambda x: x.content)  # Extract content from LLM response\n",
    "        )\n",
    "\n",
    "        # Function to combine mapped results\n",
    "        def combine_docs(docs: Sequence[str]) -> str:\n",
    "            return \"\\n\\n\".join(docs)\n",
    "        \n",
    "        # Define the reduce function\n",
    "        reduce_chain = (\n",
    "            self.reduce_prompt\n",
    "            | self.llm\n",
    "            | (lambda x: x.content)  # Extract content from LLM response\n",
    "        )\n",
    "        \n",
    "        # Create the full chain\n",
    "        chain = RunnableParallel({\n",
    "            \"documents\": lambda x: x[\"documents\"],\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "        }) | {\n",
    "            \"mapped_results\": (\n",
    "                lambda x: [{\"context\": doc.page_content, \"question\": x[\"question\"]} \n",
    "                        for doc in x[\"documents\"]]\n",
    "            ) | map_chain.map(),\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "        } | {\n",
    "            \"context\": lambda x: combine_docs(x[\"mapped_results\"]),\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "        } | reduce_chain\n",
    "\n",
    "        return chain\n",
    "    \n",
    "    async def arun(\n",
    "        self,\n",
    "        question: str,\n",
    "        documents: Optional[List[Document]] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Async run the QA chain.\"\"\"\n",
    "        if documents is None:\n",
    "            if self.retriever is None:\n",
    "                raise ValueError(\"Either documents or a retriever must be provided\")\n",
    "            documents = await self.retriever.aget_relevant_documents(question)\n",
    "        \n",
    "        return await self.chain.ainvoke({\n",
    "            \"documents\": documents,\n",
    "            \"question\": question\n",
    "        })\n",
    "    \n",
    "    def run(\n",
    "        self,\n",
    "        question: str,\n",
    "        documents: Optional[List[Document]] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Run the QA chain on a question and documents.\"\"\"\n",
    "        if documents is None:\n",
    "            if self.retriever is None:\n",
    "                raise ValueError(\"Either documents or a retriever must be provided\")\n",
    "            documents = self.retriever.get_relevant_documents(question)\n",
    "        \n",
    "        return self.chain.invoke({\n",
    "            \"documents\": documents,\n",
    "            \"question\": question\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d_/d28rwb_10zd4841zb_1t_fmw0000gn/T/ipykernel_88448/2639158125.py:121: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  documents = self.retriever.get_relevant_documents(question)\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Initialize the chain\n",
    "qa_chain = LongQAChain(\n",
    "    retriever=hybrid_retriever,\n",
    "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "answer = qa_chain.run(\"Summarise the policies of this document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The document is a Supplementary Product Disclosure Statement (SPDS) that applies to policies under the NRMA Insurance Motor NSW, ACT, TAS, and QLD Insurance Product Disclosure Statement and Policy Booklet (PDS). The SPDS provides updates to the terms contained in the PDS and should be read together with the PDS and any other applicable SPDS. It includes information on various aspects of the insurance policies, such as coverage, limits, exclusions, conditions, and benefits. Specific details are provided on Third Party Property Damage Insurance, liability cover, temporary cover, and a comparison of the 4 types of insurance offered by the company. The policy covers the cost of damage to the insured vehicle, with no excess for claims made under specified benefits. There are provisions for recovery actions in case of loss or damage, and the insured party may receive payment equal to the market value of their vehicle in certain circumstances. The document also outlines how changes can be made to the policy, who can make changes, and the policyholder's responsibilities when insured. General exclusions that apply to all covers and benefits are listed, and information on recovery actions by both the insurance company and the insured party is provided. It emphasizes the importance of understanding the policy terms, asking questions, and seeking assistance if needed.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d_/d28rwb_10zd4841zb_1t_fmw0000gn/T/ipykernel_88448/1147954633.py:45: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = short_qa_pipeline.run(query)\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import Literal, TypedDict, List\n",
    "\n",
    "# --- Setup your two different QA pipelines ---\n",
    "# Example: short QA with small K\n",
    "short_qa_pipeline = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\"),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=hybrid_retriever  # your existing retriever\n",
    ")\n",
    "\n",
    "# Example: long QA with large K or advanced summarization approach\n",
    "long_qa_pipeline = LongQAChain(\n",
    "    retriever=hybrid_retriever,\n",
    "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    ")\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    user_query: str\n",
    "    partial_answer: str\n",
    "    final_answer: str\n",
    "    approach: Literal[\"short\", \"long\"]  # chosen approach\n",
    "    done: bool\n",
    "\n",
    "# 1. Decide tool/approach\n",
    "def decide_approach(state: AgentState):\n",
    "    \"\"\"\n",
    "    Basic logic for deciding short vs. long QA.\n",
    "    Could be replaced by a classification LLM call or more advanced router.\n",
    "    \"\"\"\n",
    "    query = state[\"user_query\"]\n",
    "    if len(query) > 200 or \"summarize\" in query.lower():\n",
    "        return {\"approach\": \"long\"}\n",
    "    return {\"approach\": \"short\"}\n",
    "\n",
    "# 2. Generate answer using chosen approach\n",
    "def generate_answer(state: AgentState):\n",
    "    approach = state[\"approach\"]\n",
    "    query = state[\"user_query\"]\n",
    "    \n",
    "    if approach == \"short\":\n",
    "        answer = short_qa_pipeline.run(query)\n",
    "    else:\n",
    "        answer = long_qa_pipeline.run(query)\n",
    "    \n",
    "    return {\"partial_answer\": answer}\n",
    "\n",
    "# 3. Evaluate if we should stop or gather more context\n",
    "def evaluate_answer(state: AgentState):\n",
    "    \"\"\"\n",
    "    We'll do a basic check or optionally call an LLM to see if the answer is 'complete enough'.\n",
    "    For a simpler approach, we pass if partial_answer is \"long enough\" or\n",
    "    if the user is specifically requesting more. \n",
    "    You can replace with a tool call or any custom logic you like.\n",
    "    \"\"\"\n",
    "    partial_ans = state[\"partial_answer\"]\n",
    "    # Simple check. Replace with a real evaluator LLM if you want.\n",
    "    if len(partial_ans) > 400:\n",
    "        return {\"done\": True}\n",
    "    else:\n",
    "        # Possibly the user wants more details or the partial answer is found lacking.\n",
    "        # For demonstration, we'll do a single loop extension for “long” approach.\n",
    "        if state[\"approach\"] == \"long\":\n",
    "            return {\"done\": True}\n",
    "        return {\"done\": True}  # Or set to False for multiple loops\n",
    "\n",
    "# 4. If done, finalize answer\n",
    "def finalize_answer(state: AgentState):\n",
    "    return {\"final_answer\": state[\"partial_answer\"]}\n",
    "\n",
    "# 5. Build the Graph\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"decide_approach\", decide_approach)\n",
    "graph.add_node(\"generate_answer\", generate_answer)\n",
    "graph.add_node(\"evaluate_answer\", evaluate_answer)\n",
    "graph.add_node(\"finalize_answer\", finalize_answer)\n",
    "\n",
    "# Connect the nodes\n",
    "graph.add_edge(START, \"decide_approach\")\n",
    "graph.add_edge(\"decide_approach\", \"generate_answer\")\n",
    "graph.add_edge(\"generate_answer\", \"evaluate_answer\")\n",
    "\n",
    "# If done = True -> finalize_answer, else -> we could loop back to generate_answer\n",
    "def route_evaluation(state: AgentState):\n",
    "    if state[\"done\"]:\n",
    "        return \"Done\"\n",
    "    else:\n",
    "        return \"Repeat\"\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"evaluate_answer\",\n",
    "    route_evaluation,\n",
    "    {\n",
    "        \"Done\": \"finalize_answer\",\n",
    "        \"Repeat\": \"generate_answer\"  \n",
    "    },\n",
    ")\n",
    "\n",
    "graph.add_edge(\"finalize_answer\", END)\n",
    "\n",
    "# Compile\n",
    "agent_workflow = graph.compile()\n",
    "\n",
    "# Example usage:\n",
    "def run_agent_query(user_query: str):\n",
    "    initial_state = {\n",
    "        \"user_query\": user_query,\n",
    "        \"partial_answer\": \"\",\n",
    "        \"final_answer\": \"\",\n",
    "        \"approach\": \"short\",  # placeholder\n",
    "        \"done\": False\n",
    "    }\n",
    "\n",
    "    final_state = agent_workflow.invoke(initial_state)\n",
    "    return final_state[\"final_answer\"]\n",
    "\n",
    "answer = run_agent_query(\"What is the maximum cover for a single vehicle?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The maximum cover for a single vehicle is up to $25,000 for the agreed value '\n",
      " 'in this policy.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
